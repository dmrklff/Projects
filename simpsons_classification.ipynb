{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simpsons_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3Bp7lOuMxHm"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBytHcEINFO1"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xQsgoEhNIMI"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "from skimage import io\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "from torchvision import transforms, models\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "\n",
        "from matplotlib import colors, pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_eRsXLlNOOd"
      },
      "source": [
        "DATA_MODES = ['train', 'val', 'test']\n",
        "\n",
        "RESCALE_SIZE = 224\n",
        "\n",
        "DEVICE = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWOSyN0HNS61"
      },
      "source": [
        "class SimpsonsDataset(Dataset):\n",
        "    def __init__(self, files, mode):\n",
        "        super().__init__()\n",
        "        self.files = sorted(files)\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode not in DATA_MODES:\n",
        "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
        "            raise NameError\n",
        "\n",
        "        self.len_ = len(self.files)\n",
        "     \n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        if self.mode != 'test':\n",
        "            self.labels = [path.parent.name for path in self.files]\n",
        "            self.label_encoder.fit(self.labels)\n",
        "\n",
        "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
        "                  pickle.dump(self.label_encoder, le_dump_file)\n",
        "                      \n",
        "    def __len__(self):\n",
        "        return self.len_\n",
        "      \n",
        "    def load_sample(self, file):\n",
        "        image = Image.open(file)\n",
        "        image.load()\n",
        "        return image\n",
        "  \n",
        "    def __getitem__(self, index):         \n",
        "        transform = {\n",
        "        'train': transforms.Compose([\n",
        "        transforms.Resize(RESCALE_SIZE),\n",
        "        transforms.CenterCrop(RESCALE_SIZE),\n",
        "        transforms.RandomChoice( [ \n",
        "                                  transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                  transforms.RandomRotation(degrees=40),\n",
        "                                  transforms.ColorJitter(brightness=0.1),\n",
        "                                  transforms.RandomApply( [ transforms.RandomRotation(degrees=40), transforms.ColorJitter(contrast=0.9) ], p=0.5),\n",
        "                                  transforms.RandomApply( [ transforms.RandomRotation(degrees=40), transforms.RandomRotation(degrees=10) ], p=0.5),\n",
        "                                  ] ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "        transforms.Resize(RESCALE_SIZE),\n",
        "        transforms.CenterCrop(RESCALE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'test': transforms.Compose([\n",
        "        transforms.Resize(RESCALE_SIZE),\n",
        "        transforms.CenterCrop(RESCALE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        }\n",
        "\n",
        "\n",
        "        x = self.load_sample(self.files[index])\n",
        "        x = transform[self.mode](x)\n",
        "        if self.mode == 'test':\n",
        "            return x\n",
        "        else:\n",
        "            label = self.labels[index]\n",
        "            label_id = self.label_encoder.transform([label])\n",
        "            y = label_id.item()\n",
        "            return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyAuNFjtNa91"
      },
      "source": [
        "def imshow(inp, title=None, plt_ax=plt, default=False):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt_ax.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt_ax.set_title(title)\n",
        "    plt_ax.grid(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XMFBFSuNeD4"
      },
      "source": [
        "TRAIN_DIR = Path('../input/journey-springfield/train/simpsons_dataset')\n",
        "TEST_DIR = Path('../input/journey-springfield/testset/testset')\n",
        "\n",
        "train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
        "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgZFtWScNggU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_val_labels = [path.parent.name for path in train_val_files]\n",
        "train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n",
        "                                          stratify=train_val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2kszm86NkpK"
      },
      "source": [
        "train_labels = [path.parent.name for path in train_files]\n",
        "val_labels = [path.parent.name for path in val_files]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjdeyr-1Nnul"
      },
      "source": [
        "def create_dct_path_labels(train_files, train_labels):\n",
        "    dct_simpsons = {}\n",
        "    for label_i in np.unique(train_labels).tolist():\n",
        "        dct_simpsons[label_i] = []\n",
        "\n",
        "    for path_i, label_i in zip(train_files, train_labels):\n",
        "        dct_simpsons[label_i].append(path_i)\n",
        "\n",
        "    return dct_simpsons\n",
        "\n",
        "def print_dct(dct_simpsons):\n",
        "    for key in dct_simpsons:\n",
        "        print(f\"{key}\\t{dct_simpsons[key]}\")\n",
        "\n",
        "def create_dct_from_labels(train_val_labels):\n",
        "    dct_simpsons = {}\n",
        "    for label_i in np.unique(train_val_labels).tolist():\n",
        "        dct_simpsons.update({label_i:train_val_labels.count(label_i)})\n",
        "\n",
        "    return dct_simpsons\n",
        "        \n",
        "dct_path_train = create_dct_path_labels(train_files, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tANx5lpkNrHj"
      },
      "source": [
        "for person in dct_path_train:\n",
        "    if len(dct_path_train[person]) < 100:\n",
        "        dct_path_train[person] = dct_path_train[person] * (100 // len(dct_path_train[person]))\n",
        "        dct_path_train[person].extend(dct_path_train[person][:100 - len(dct_path_train[person])])\n",
        "        \n",
        "new_train_files = []\n",
        "\n",
        "for person in dct_path_train:\n",
        "    new_train_files.extend(dct_path_train[person])\n",
        "\n",
        "new_train_label = [path.parent.name for path in new_train_files]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKXVtfQRNxbV"
      },
      "source": [
        "val_dataset = SimpsonsDataset(val_files, mode='val')\n",
        "new_train_dataset = SimpsonsDataset(new_train_files, mode='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyLqbo4wN6UW"
      },
      "source": [
        "dataloaders_dict = {'train': DataLoader(new_train_dataset, batch_size=64, shuffle=True, num_workers=4),\n",
        "                          'val': DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onn15W6jN_f-"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(8, 8), \\\n",
        "                        sharey=True, sharex=True)\n",
        "for fig_x in ax.flatten():\n",
        "    random_characters = int(np.random.uniform(0,1000))\n",
        "    im_val, label = new_train_dataset[random_characters]\n",
        "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
        "                new_train_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n",
        "    imshow(im_val.data.cpu(), \\\n",
        "          title=img_label,plt_ax=fig_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1mEsXzaOCBm"
      },
      "source": [
        "def fit_epoch(model, train_loader, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_data = 0\n",
        "  \n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_data += inputs.size(0)\n",
        "              \n",
        "    train_loss = running_loss / processed_data\n",
        "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
        "    return train_loss, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJyUvbwrOMUl"
      },
      "source": [
        "def eval_epoch(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_size = 0\n",
        "\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_size += inputs.size(0)\n",
        "    val_loss = running_loss / processed_size\n",
        "    val_acc = running_corrects.double() / processed_size\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkPxe8uIOPeL"
      },
      "source": [
        "def train(train_files, val_files, model, epochs, batch_size, learning_rate):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    history = []\n",
        "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
        "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
        "\n",
        "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
        "        opt = torch.optim.AdamW(model.parameters())\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, \"min\")\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
        "            print(\"loss\", train_loss)\n",
        "            \n",
        "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
        "            \n",
        "            scheduler.step(val_loss)\n",
        "            pbar_outer.update(1)\n",
        "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
        "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
        "            \n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr8QMwFEOR96"
      },
      "source": [
        "def predict(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "    \n",
        "        for inputs in test_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            model.eval()\n",
        "            outputs = model(inputs).cpu()\n",
        "            logits.append(outputs)\n",
        "            \n",
        "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swhtocr4OUEE"
      },
      "source": [
        "n_classes = len(np.unique(train_val_labels))\n",
        "print(\"we will classify :{}\".format(n_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br5vtgBJOWPX"
      },
      "source": [
        "if val_dataset is None:\n",
        "    val_dataset = SimpsonsDataset(val_files, mode='val')\n",
        "    \n",
        "train_dataset = SimpsonsDataset(train_files, mode='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu7xxDGbOYXo"
      },
      "source": [
        "import math\n",
        "def find_lr(model, dataloaders, loss_fn, optimizer, init_value=1e-8, final_value=10.0, use_gpu=True):\n",
        "    model.train()\n",
        "    number_in_epoch = len(dataloaders['train']) - 1\n",
        "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
        "    lr = init_value\n",
        "    optimizer.param_groups[0][\"lr\"] = lr\n",
        "    best_loss = 0.0\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "    for inputs, labels in dataloaders['train']:\n",
        "        if use_gpu:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "        batch_num += 1\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "\n",
        "        if batch_num > 1 and loss > 4 * best_loss:\n",
        "            return log_lrs[10:-5], losses[10:-5]\n",
        "\n",
        "\n",
        "        if loss < best_loss or batch_num == 1:\n",
        "            best_loss = loss\n",
        "\n",
        "\n",
        "        losses.append(loss)\n",
        "        log_lrs.append(math.log10(lr))\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        lr *= update_step\n",
        "        optimizer.param_groups[0][\"lr\"] = lr\n",
        "    return log_lrs[10:-5], losses[10:-5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYSnyS4COfdA"
      },
      "source": [
        "resnet50_model = models.resnet50(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bndbh1pOhkG"
      },
      "source": [
        "for param in resnet50_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "resnet50_model.fc = nn.Sequential(nn.Linear(2048, 128),\n",
        "                                nn.LeakyReLU(inplace=True),\n",
        "                                nn.Linear(128, n_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIJPk9bGOjyQ"
      },
      "source": [
        "params_to_update = resnet50_model.parameters()\n",
        "print(\"Params to learn:\")\n",
        "\n",
        "params_to_update = []\n",
        "for name,param in resnet50_model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo4oE6f4OmGO"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss() \n",
        "\n",
        "optimizer = torch.optim.AdamW(params_to_update, lr=1e-3, amsgrad=True)\n",
        "\n",
        "logs, losses = find_lr(resnet50_model, dataloaders_dict, loss_fn, optimizer, init_value=1e-8, final_value=10.0)\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(logs,losses)\n",
        "ax.set_xlabel(\"$10^x$\")\n",
        "ax.set_ylabel(\"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3dVTVfaOoq6"
      },
      "source": [
        "resnet50_model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WeGSh22O4hM"
      },
      "source": [
        "history = train(train_dataset, val_dataset, model=resnet50_model, epochs=25, batch_size=32, learning_rate=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Ss-JO0O6o_"
      },
      "source": [
        "loss, acc, val_loss, val_acc = zip(*history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgW4mjQtO8by"
      },
      "source": [
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(loss, label=\"train_loss\")\n",
        "plt.plot(val_loss, label=\"val_loss\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(acc, label=\"train_acc\")\n",
        "plt.plot(val_acc, label=\"val_acc\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"acc\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrXy-q-AO-VY"
      },
      "source": [
        "for param in resnet50_model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "params_to_update = resnet50_model.parameters()\n",
        "print(\"Params to learn:\")\n",
        "\n",
        "\n",
        "\n",
        "params_to_update = []\n",
        "for name,param in resnet50_model.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnEP9IGxPCfZ"
      },
      "source": [
        "history = train(train_dataset, val_dataset, model=resnet50_model, epochs=15, batch_size=32, learning_rate=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpJhByKgPI7h"
      },
      "source": [
        "loss, acc, val_loss, val_acc = zip(*history)\n",
        "\n",
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(loss, label=\"train_loss\")\n",
        "plt.plot(val_loss, label=\"val_loss\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(acc, label=\"train_acc\")\n",
        "plt.plot(val_acc, label=\"val_acc\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"acc\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-idyaa3PLSr"
      },
      "source": [
        "def predict_one_sample(model, inputs, device=DEVICE):\n",
        "    with torch.no_grad():\n",
        "        inputs = inputs.to(device)\n",
        "        model.eval()\n",
        "        logit = model(inputs).cpu()\n",
        "        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n",
        "    return probs\n",
        "\n",
        "random_characters = int(np.random.uniform(0,1000))\n",
        "ex_img, true_label = val_dataset[random_characters]\n",
        "probs_im = predict_one_sample(resnet50_model, ex_img.unsqueeze(0))\n",
        "\n",
        "idxs = list(map(int, np.random.uniform(0,1000, 20)))\n",
        "imgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n",
        "\n",
        "probs_ims = predict(resnet50_model, imgs)\n",
        "\n",
        "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n",
        "\n",
        "y_pred = np.argmax(probs_ims,-1)\n",
        "\n",
        "actual_labels = [val_dataset[id][1] for id in idxs]\n",
        "\n",
        "preds_class = [label_encoder.classes_[i] for i in y_pred]\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(actual_labels, y_pred, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6OuaRdXPQIl"
      },
      "source": [
        "test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
        "probs = predict(resnet50_model, test_loader)\n",
        "\n",
        "preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
        "test_filenames = [path.name for path in test_dataset.files]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl6BuvuMPWbs"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df['Id'] = test_filenames\n",
        "df['Expected'] = preds\n",
        "df.to_csv('model_55_leaky_4.csv', index=False)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}